{
  "lastUpdated": "2026-02-24T02:45:21.544886",
  "topics": {
    "ai_ml": {
      "name": "AI & Machine Learning",
      "count": 10
    },
    "causal": {
      "name": "Causal Inference",
      "count": 10
    },
    "llm": {
      "name": "Large Language Models",
      "count": 5
    },
    "reinforcement": {
      "name": "Reinforcement Learning",
      "count": 5
    }
  },
  "papers": [
    {
      "title": "Smooth Gate Functions for Soft Advantage Policy Optimization",
      "authors": "Egor Denisov, Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko",
      "abstract": "Group Relative Policy Optimization (GRPO) has significantly advanced the training of large language models and enhanced their reasoning capabilities, while it remains susceptible to instability due to the use of hard clipping. Soft Adaptive Policy Optimization (SAPO) addresses this limitation by replacing clipping with a smooth sigmoid-based gate function, which leads to more stable updates. We have decided to push this theory further and investigate the impact of different gate functions on both training stability and final model performance. We formalize the key properties that admissible gates should satisfy and identify several families of such functions for empirical evaluation. This paper presents an analysis of our findings based on experiments conducted with the Qwen2.5-7B-Instruct model on mathematical reasoning tasks. These results provide practical guidance for designing smoother and more robust policy optimization objectives for large language model training.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19345v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19345v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits",
      "authors": "Anna Volodkevich, Dmitry Anikin, Danil Gusak, Anton Klenitskiy, Evgeny Frolov, et al.",
      "abstract": "Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.   In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19339v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19339v1.pdf",
      "categories": [
        "cs.IR",
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "PerSoMed: A Large-Scale Balanced Dataset for Persian Social Media Text Classification",
      "authors": "Isun Chehreh, Ebrahim Ansari",
      "abstract": "This research introduces the first large-scale, well-balanced Persian social media text classification dataset, specifically designed to address the lack of comprehensive resources in this domain. The dataset comprises 36,000 posts across nine categories (Economic, Artistic, Sports, Political, Social, Health, Psychological, Historical, and Science & Technology), each containing 4,000 samples to ensure balanced class distribution. Data collection involved 60,000 raw posts from various Persian social media platforms, followed by rigorous preprocessing and hybrid annotation combining ChatGPT-based few-shot prompting with human verification. To mitigate class imbalance, we employed undersampling with semantic redundancy removal and advanced data augmentation strategies integrating lexical replacement and generative prompting. We benchmarked several models, including BiLSTM, XLM-RoBERTa (with LoRA and AdaLoRA adaptations), FaBERT, SBERT-based architectures, and the Persian-specific TookaBERT (Base and Large). Experimental results show that transformer-based models consistently outperform traditional neural networks, with TookaBERT-Large achieving the best performance (Precision: 0.9622, Recall: 0.9621, F1- score: 0.9621). Class-wise evaluation further confirms robust performance across all categories, though social and political texts exhibited slightly lower scores due to inherent ambiguity. This research presents a new high-quality dataset and provides comprehensive evaluations of cutting-edge models, establishing a solid foundation for further developments in Persian NLP, including trend analysis, social behavior modeling, and user classification. The dataset is publicly available to support future research endeavors.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19333v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19333v1.pdf",
      "categories": [
        "cs.CL",
        "cs.IR",
        "cs.SI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Training-Free Cross-Architecture Merging for Graph Neural Networks",
      "authors": "Rishabh Bhattacharya, Vikaskumar Kalsariya, Naresh Manwani",
      "abstract": "Model merging has emerged as a powerful paradigm for combining the capabilities of distinct expert models without the high computational cost of retraining, yet current methods are fundamentally constrained to homogeneous architectures. For GNNs, however, message passing is topology-dependent and sensitive to misalignment, making direct parameter-space merging unreliable. To bridge this gap, we introduce H-GRAMA (Heterogeneous Graph Routing and Message Alignment), a training-free framework that lifts merging from parameter space to operator space. We formalize Universal Message Passing Mixture (UMPM), a shared operator family that expresses heterogeneous GNN layers in a common functional language. H-GRAMA enables cross-architecture GNN merging (e.g., GCN to GAT) without retraining, retaining high specialist accuracy in most cases in compatible depth settings and achieving inference speedups of 1.2x to 1.9x over ensembles.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19332v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19332v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Partial Soft-Matching Distance for Neural Representational Comparison with Partial Unit Correspondence",
      "authors": "Chaitanya Kapoor, Alex H. Williams, Meenakshi Khosla",
      "abstract": "Representational similarity metrics typically force all units to be matched, making them susceptible to noise and outliers common in neural representations. We extend the soft-matching distance to a partial optimal transport setting that allows some neurons to remain unmatched, yielding rotation-sensitive but robust correspondences. This partial soft-matching distance provides theoretical advantages -- relaxing strict mass conservation while maintaining interpretable transport costs -- and practical benefits through efficient neuron ranking in terms of cross-network alignment without costly iterative recomputation. In simulations, it preserves correct matches under outliers and reliably selects the correct model in noise-corrupted identification tasks. On fMRI data, it automatically excludes low-reliability voxels and produces voxel rankings by alignment quality that closely match computationally expensive brute-force approaches. It achieves higher alignment precision across homologous brain areas than standard soft-matching, which is forced to match all units regardless of quality. In deep networks, highly matched units exhibit similar maximally exciting images, while unmatched units show divergent patterns. This ability to partition by match quality enables focused analyses, e.g., testing whether networks have privileged axes even within their most aligned subpopulations. Overall, partial soft-matching provides a principled and practical method for representational comparison under partial correspondence.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19331v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19331v1.pdf",
      "categories": [
        "cs.LG",
        "cs.NE",
        "stat.ML"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "CTS-Bench: Benchmarking Graph Coarsening Trade-offs for GNNs in Clock Tree Synthesis",
      "authors": "Barsat Khadka, Kawsher Roxy, Md Rubel Ahmed",
      "abstract": "Graph Neural Networks (GNNs) are increasingly explored for physical design analysis in Electronic Design Automation, particularly for modeling Clock Tree Synthesis behavior such as clock skew and buffering complexity. However, practical deployment remains limited due to the prohibitive memory and runtime cost of operating on raw gate-level netlists. Graph coarsening is commonly used to improve scalability, yet its impact on CTS-critical learning objectives is not well characterized. This paper introduces CTS-Bench, a benchmark suite for systematically evaluating the trade-offs between graph coarsening, prediction accuracy, and computational efficiency in GNN-based CTS analysis. CTS-Bench consists of 4,860 converged physical design solutions spanning five architectures and provides paired raw gate-level and clustered graph representations derived from post-placement designs. Using clock skew prediction as a representative CTS task, we demonstrate a clear accuracy-efficiency trade-off. While graph coarsening reduces GPU memory usage by up to 17.2x and accelerates training by up to 3x, it also removes structural information essential for modeling clock distribution, frequently resulting in negative $R^2$ scores under zero-shot evaluation. Our findings indicate that generic graph clustering techniques can fundamentally compromise CTS learning objectives, even when global physical metrics remain unchanged. CTS-Bench enables principled evaluation of CTS-aware graph coarsening strategies, supports benchmarking of GNN architectures and accelerators under realistic physical design constraints, and provides a foundation for developing learning-assisted CTS analysis and optimization techniques.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19330v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19330v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Dynamic Elasticity Between Forest Loss and Carbon Emissions: A Subnational Panel Analysis of the United States",
      "authors": "Keonvin Park",
      "abstract": "Accurate quantification of the relationship between forest loss and associated carbon emissions is critical for both environmental monitoring and policy evaluation. Although many studies have documented spatial patterns of forest degradation, there is limited understanding of the dynamic elasticity linking tree cover loss to carbon emissions at subnational scales. In this paper, we construct a comprehensive panel dataset of annual forest loss and carbon emission estimates for U.S. subnational administrative units from 2001 to 2023, based on the Hansen Global Forest Change dataset. We apply fixed effects and dynamic panel regression techniques to isolate within-region variation and account for temporal persistence in emissions. Our results show that forest loss has a significant positive short-run elasticity with carbon emissions, and that emissions exhibit strong persistence over time. Importantly, the estimated long-run elasticity, accounting for autoregressive dynamics, is substantially larger than the short-run effect, indicating cumulative impacts of repeated forest loss events. These findings highlight the importance of modeling temporal dynamics when assessing environmental responses to land cover change. The dynamic elasticity framework proposed here offers a robust and interpretable tool for analyzing environmental change processes, and can inform both regional monitoring systems and carbon accounting frameworks.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19329v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19329v1.pdf",
      "categories": [
        "stat.AP",
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Soft Sequence Policy Optimization: Bridging GMPO and SAPO",
      "authors": "Svetlana Glazyrina, Maksim Kryzhanovskiy, Roman Ischenko",
      "abstract": "A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19327v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19327v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification",
      "authors": "Rui Liu, Steven Jige Quan, Zhong-Ren Peng, Zijun Yao, Han Wang, et al.",
      "abstract": "As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual effort to redraw geospatial layouts, slowing the iterative planning and decision-making procedure. Motivated by recent advances in agentic systems and multimodal reasoning, we formulate urban renewal as a machine-executable task that iteratively modifies existing urban plans represented in structured geospatial formats. More specifically, we represent urban layouts using GeoJSON and decompose natural-language editing instructions into hierarchical geometric intents spanning polygon-, line-, and point-level operations. To coordinate interdependent edits across spatial elements and abstraction levels, we propose a hierarchical agentic framework that jointly performs multi-level planning and execution with explicit propagation of intermediate spatial constraints. We further introduce an iterative execution-validation mechanism that mitigates error accumulation and enforces global spatial consistency during multi-step editing. Extensive experiments across diverse urban editing scenarios demonstrate significant improvements in efficiency, robustness, correctness, and spatial validity over existing baselines.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19326v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19326v1.pdf",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "RetinaVision: XAI-Driven Augmented Regulation for Precise Retinal Disease Classification using deep learning framework",
      "authors": "Mohammad Tahmid Noor, Shayan Abrar, Jannatul Adan Mahi, Md Parvez Mia, Asaduzzaman Hridoy, et al.",
      "abstract": "Early and accurate classification of retinal diseases is critical to counter vision loss and for guiding clinical management of retinal diseases. In this study, we proposed a deep learning method for retinal disease classification utilizing optical coherence tomography (OCT) images from the Retinal OCT Image Classification - C8 dataset (comprising 24,000 labeled images spanning eight conditions). Images were resized to 224x224 px and tested on convolutional neural network (CNN) architectures: Xception and InceptionV3. Data augmentation techniques (CutMix, MixUp) were employed to enhance model generalization. Additionally, we applied GradCAM and LIME for interpretability evaluation. We implemented this in a real-world scenario via our web application named RetinaVision. This study found that Xception was the most accurate network (95.25%), followed closely by InceptionV3 (94.82%). These results suggest that deep learning methods allow effective OCT retinal disease classification and highlight the importance of implementing accuracy and interpretability for clinical applications.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19324v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19324v1.pdf",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Attention Deficits in Language Models: Causal Explanations for Procedural Hallucinations",
      "authors": "Ahmed Karim, Fatima Sheaib, Zein Khamis, Maggie Chlon, Jad Awada, et al.",
      "abstract": "Large language models can follow complex procedures yet fail at a seemingly trivial final step: reporting a value they themselves computed moments earlier. We study this phenomenon as \\emph{procedural hallucination}: failure to execute a verifiable, prompt-grounded specification even when the correct value is present in context.   In long-context binding tasks with a known single-token candidate set, we find that many errors are readout-stage routing failures. Specifically, failures decompose into Stage~2A (gating) errors, where the model does not enter answer mode, and Stage~2B (binding) errors, where it enters answer mode but selects the wrong candidate (often due to recency bias). In the hard regime, Stage~2B accounts for most errors across model families in our tasks (Table~1).   On Stage~2B error trials, a linear probe on the final-layer residual stream recovers the correct value far above chance (e.g., 74\\% vs.\\ 2\\% on Qwen2.5-3B; Table~2), indicating that the answer is encoded but not used. We formalize ``present but not used'' via available vs.\\ used mutual information and pseudo-prior interventions, yielding output-computable diagnostics and information-budget certificates.   Finally, an oracle checkpointing intervention that restates the true binding near the query can nearly eliminate Stage~2B failures at long distance (e.g., Qwen2.5-3B $0/400 \\rightarrow 399/400$ at $k = 1024$; Table~8).",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19239v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19239v1.pdf",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM",
      "authors": "Francesca Bianco, Derek Shiller",
      "abstract": "Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19159v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19159v1.pdf",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "DoAtlas-1: A Causal Compilation Paradigm for Clinical AI",
      "authors": "Yulong Li, Jianxu Chen, Xiwei Liu, Chuanyue Suo, Rong Xia, et al.",
      "abstract": "Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19158v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19158v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians",
      "authors": "Kartik Chandra, Max Kleiman-Weiner, Jonathan Ragan-Kelley, Joshua B. Tenenbaum",
      "abstract": "\"AI psychosis\" or \"delusional spiraling\" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called \"sycophancy.\" In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19141v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19141v1.pdf",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Test-Time Learning of Causal Structure from Interventional Data",
      "authors": "Wei Chen, Rui Ding, Bojun Huang, Yang Zhang, Qiang Fu, et al.",
      "abstract": "Supervised causal learning has shown promise in causal discovery, yet it often struggles with generalization across diverse interventional settings, particularly when intervention targets are unknown. To address this, we propose TICL (Test-time Interventional Causal Learning), a novel method that synergizes Test-Time Training with Joint Causal Inference. Specifically, we design a self-augmentation strategy to generate instance-specific training data at test time, effectively avoiding distribution shifts. Furthermore, by integrating joint causal inference, we developed a PC-inspired two-phase supervised learning scheme, which effectively leverages self-augmented training data while ensuring theoretical identifiability. Extensive experiments on bnlearn benchmarks demonstrate TICL's superiority in multiple aspects of causal discovery and intervention target detection.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19131v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19131v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions",
      "authors": "Yao Yan",
      "abstract": "We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how   arithmetic answers are finalized after cross-token routing becomes causally irrelevant.   Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:   beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention   is largely dispensable.   In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are   well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).   Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the   learned map restores strict counterfactual edits; negative controls do not recover.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19109v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19109v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks",
      "authors": "Wilson Y. Lee",
      "abstract": "Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\\hatÎ²=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19008v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19008v1.pdf",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Incremental Transformer Neural Processes",
      "authors": "Philip Mortimer, Cristiana Diaconu, Tommy Rochussen, Bruno Mlodozeniec, Richard E. Turner",
      "abstract": "Neural Processes (NPs), and specifically Transformer Neural Processes (TNPs), have demonstrated remarkable performance across tasks ranging from spatiotemporal forecasting to tabular data modelling. However, many of these applications are inherently sequential, involving continuous data streams such as real-time sensor readings or database updates. In such settings, models should support cheap, incremental updates rather than recomputing internal representations from scratch for every new observation -- a capability existing TNP variants lack. Drawing inspiration from Large Language Models, we introduce the Incremental TNP (incTNP). By leveraging causal masking, Key-Value (KV) caching, and a data-efficient autoregressive training strategy, incTNP matches the predictive performance of standard TNPs while reducing the computational cost of updates from quadratic to linear time complexity. We empirically evaluate our model on a range of synthetic and real-world tasks, including tabular regression and temperature prediction. Our results show that, surprisingly, incTNP delivers performance comparable to -- or better than -- non-causal TNPs while unlocking orders-of-magnitude speedups for sequential inference. Finally, we assess the consistency of the model's updates -- by adapting a metric of ``implicit Bayesianness\", we show that incTNP retains a prediction rule as implicitly Bayesian as standard non-causal TNPs, demonstrating that incTNP achieves the computational benefits of causal masking without sacrificing the consistency required for streaming inference.",
      "published": "2026-02-21",
      "link": "http://arxiv.org/abs/2602.18955v1",
      "pdfLink": "http://arxiv.org/pdf/2602.18955v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "TRUE: A Trustworthy Unified Explanation Framework for Large Language Model Reasoning",
      "authors": "Yujiao Yang",
      "abstract": "Large language models (LLMs) have demonstrated strong capabilities in complex reasoning tasks, yet their decision-making processes remain difficult to interpret. Existing explanation methods often lack trustworthy structural insight and are limited to single-instance analysis, failing to reveal reasoning stability and systematic failure mechanisms. To address these limitations, we propose the Trustworthy Unified Explanation Framework (TRUE), which integrates executable reasoning verification, feasible-region directed acyclic graph (DAG) modeling, and causal failure mode analysis. At the instance level, we redefine reasoning traces as executable process specifications and introduce blind execution verification to assess operational validity. At the local structural level, we construct feasible-region DAGs via structure-consistent perturbations, enabling explicit characterization of reasoning stability and the executable region in the local input space. At the class level, we introduce a causal failure mode analysis method that identifies recurring structural failure patterns and quantifies their causal influence using Shapley values. Extensive experiments across multiple reasoning benchmarks demonstrate that the proposed framework provides multi-level, verifiable explanations, including executable reasoning structures for individual instances, feasible-region representations for neighboring inputs, and interpretable failure modes with quantified importance at the class level. These results establish a unified and principled paradigm for improving the interpretability and reliability of LLM reasoning systems.",
      "published": "2026-02-21",
      "link": "http://arxiv.org/abs/2602.18905v1",
      "pdfLink": "http://arxiv.org/pdf/2602.18905v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Bounds and Identification of Joint Probabilities of Potential Outcomes and Observed Variables under Monotonicity Assumptions",
      "authors": "Naoya Hashimoto, Yuta Kawakami, Jin Tian",
      "abstract": "Evaluating joint probabilities of potential outcomes and observed variables, and their linear combinations, is a fundamental challenge in causal inference. This paper addresses the bounding and identification of these probabilities in settings with discrete treatment and discrete ordinal outcome. We propose new families of monotonicity assumptions and formulate the bounding problem as a linear programming problem. We further introduce a new monotonicity assumption specifically to achieve identification. Finally, we present numerical experiments to validate our methods and demonstrate their application using real-world datasets.",
      "published": "2026-02-21",
      "link": "http://arxiv.org/abs/2602.18762v1",
      "pdfLink": "http://arxiv.org/pdf/2602.18762v1.pdf",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations",
      "authors": "Dongming Jiang, Yi Li, Songtao Wei, Jinxin Yang, Ayushi Kishore, et al.",
      "abstract": "Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19320v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19320v1.pdf",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering",
      "authors": "Maryam Amirizaniani, Alireza Salemi, Hamed Zamani",
      "abstract": "Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19317v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19317v1.pdf",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "A Power Market Model with Hypersaclers and Modular Datacenters",
      "authors": "Yihsu Chen, Abel Souza, Fargol Nematkhah, Andrew L. Liu",
      "abstract": "The rapid adoption of AI has led the growth of computational demand, with large language models (LLMs) at the forefront since ChatGPT's debut in 2022. Meanwhile, large amounts of renewable energy have been deployed but, ultimately, curtailed due to transmission congestion and inadequate demand. This work develops a power market model that allows hyperscalers to spatially migrate LLM inference workloads to geo-distributed modular datacenters (MDCs), which are co-located with near renewable sources of energy at the edge of the network. We introduce the optimization problems faced by the hyperscaler and MDCs in addition to consumers, producers, and the electric grid operator, where the hyerscaler enters an agreement to lease MDCs while ensuring that the required service level objectives (SLOs) are met. The overall market model is formulated as a complementarity problem, where the proof is provided showing the existence and uniqueness of the solutions. When applying the model to an IEEE RTS-24 bus case study, we show that even with a provision that requires MDCs to disclose the CO$_2$ emissions associated with their energy supply sources, renting less polluting MDCs is unlikely to yield meaningful emission reductions due to so-called contract-reshuffling. The situation can be mitigated when conventional loads are supplied by forward contracts through power purchase agreements. This also leads to a decline in system congestion when the hyperscaler becomes increasingly cost-aware.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19310v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19310v1.pdf",
      "categories": [
        "eess.SY"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
      "authors": "Shirui Chen, Cole Harrison, Ying-Chun Lee, Angela Jin Yang, Zhongzheng Ren, et al.",
      "abstract": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19313v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19313v1.pdf",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease",
      "authors": "Nolan Brady, Tom Yeh",
      "abstract": "Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19298v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19298v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "Dynamic Repair and Maintenance of Heterogeneous Machines Dispersed on a Network: A Rollout Method for Online Reinforcement Learning",
      "authors": "Dongnuan Tian, Rob Shone",
      "abstract": "We consider a problem in which a single repairer is responsible for the maintenance and repair of a collection of machines, positioned at different locations on a network of nodes and edges. Machines deteriorate according to stochastic processes and incur increasing costs as they approach complete failure. The times needed for repairs to be performed, and the amounts of time needed for the repairer to switch between different machines, are random and machine-dependent. The problem is formulated as a Markov decision process (MDP) in which the objective is to minimize long-run average costs. We prove the equivalence of an alternative formulation based on rewards and use this to develop an index heuristic policy, which is shown to be optimal in certain special cases. We then use rollout-based reinforcement learning techniques to develop a novel online policy improvement (OPI) approach, which uses the index heuristic as a base policy and also as an insurance option at decision epochs where the best action cannot be selected with sufficient confidence. Results from extensive numerical experiments, involving randomly-generated network layouts and parameter values, show that the OPI heuristic is able to achieve close-to-optimal performance in fast-changing systems with state transitions occurring 100 times per second, suggesting that it is suitable for online implementation.",
      "published": "2026-02-22",
      "link": "http://arxiv.org/abs/2602.19277v1",
      "pdfLink": "http://arxiv.org/pdf/2602.19277v1.pdf",
      "categories": [
        "math.OC"
      ],
      "topic": "Reinforcement Learning"
    }
  ]
}