{
  "lastUpdated": "2026-02-25T02:46:20.410377",
  "topics": {
    "ai_ml": {
      "name": "AI & Machine Learning",
      "count": 10
    },
    "causal": {
      "name": "Causal Inference",
      "count": 10
    },
    "llm": {
      "name": "Large Language Models",
      "count": 5
    },
    "reinforcement": {
      "name": "Reinforcement Learning",
      "count": 5
    }
  },
  "papers": [
    {
      "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
      "authors": "Vishal Patil, Shree Vaishnavi Bacha, Revanth Yamani, Yidan Sun, Mayank Kejriwal",
      "abstract": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21082v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21082v1.pdf",
      "categories": [
        "cs.CL"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Scaling Vision Transformers: Evaluating DeepSpeed for Image-Centric Workloads",
      "authors": "Huy Trinh, Rebecca Ma, Zeqi Yu, Tahsin Reza",
      "abstract": "Vision Transformers (ViTs) have demonstrated remarkable potential in image processing tasks by utilizing self-attention mechanisms to capture global relationships within data. However, their scalability is hindered by significant computational and memory demands, especially for large-scale models with many parameters. This study aims to leverage DeepSpeed, a highly efficient distributed training framework that is commonly used for language models, to enhance the scalability and performance of ViTs. We evaluate intra- and inter-node training efficiency across multiple GPU configurations on various datasets like CIFAR-10 and CIFAR-100, exploring the impact of distributed data parallelism on training speed, communication overhead, and overall scalability (strong and weak scaling). By systematically varying software parameters, such as batch size and gradient accumulation, we identify key factors influencing performance of distributed training. The experiments in this study provide a foundational basis for applying DeepSpeed to image-related tasks. Future work will extend these investigations to deepen our understanding of DeepSpeed's limitations and explore strategies for optimizing distributed training pipelines for Vision Transformers.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21081v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21081v1.pdf",
      "categories": [
        "cs.LG",
        "eess.SP"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning",
      "authors": "Duowen Chen, Yan Wang",
      "abstract": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21078v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21078v1.pdf",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Localized Dynamics-Aware Domain Adaption for Off-Dynamics Offline Reinforcement Learning",
      "authors": "Zhangjie Xia, Yu Yang, Pan Xu",
      "abstract": "Off-dynamics offline reinforcement learning (RL) aims to learn a policy for a target domain using limited target data and abundant source data collected under different transition dynamics. Existing methods typically address dynamics mismatch either globally over the state space or via pointwise data filtering; these approaches can miss localized cross-domain similarities or incur high computational cost. We propose Localized Dynamics-Aware Domain Adaptation (LoDADA), which exploits localized dynamics mismatch to better reuse source data. LoDADA clusters transitions from source and target datasets and estimates cluster-level dynamics discrepancy via domain discrimination. Source transitions from clusters with small discrepancy are retained, while those from clusters with large discrepancy are filtered out. This yields a fine-grained and scalable data selection strategy that avoids overly coarse global assumptions and expensive per-sample filtering. We provide theoretical insights and extensive experiments across environments with diverse global and local dynamics shifts. Results show that LoDADA consistently outperforms state-of-the-art off-dynamics offline RL methods by better leveraging localized distribution mismatch.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21072v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21072v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "The Initial Exploration Problem in Knowledge Graph Exploration",
      "authors": "Claire McNamara, Lucy Hederman, Declan O'Sullivan",
      "abstract": "Knowledge Graphs (KGs) enable the integration and representation of complex information across domains, but their semantic richness and structural complexity create substantial barriers for lay users without expertise in semantic web technologies. When encountering an unfamiliar KG, such users face a distinct orientation challenge: they do not know what questions are possible, how the knowledge is structured, or how to begin exploration. This paper identifies and theorises this phenomenon as the Initial Exploration Problem (IEP). Drawing on theories from information behaviour and human-computer interaction, including ASK, exploratory search, information foraging, and cognitive load theory, we develop a conceptual framing of the IEP characterised by three interdependent barriers: scope uncertainty, ontology opacity, and query incapacity. We argue that these barriers converge at the moment of first contact, distinguishing the IEP from related concepts that presuppose an existing starting point or information goal. Analysing KG exploration interfaces at the level of interaction primitives, we suggest that many systems rely on epistemic assumptions that do not hold at first contact. This reveals a structural gap in the design space: the absence of interaction primitives for scope revelation, mechanisms that communicate what a KG contains without requiring users to formulate queries or interpret ontological structures. In articulating the IEP, this paper provides a theoretical lens for evaluating KG interfaces and for designing entry-point scaffolding that supports initial exploration.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21066v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21066v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Motivation is Something You Need",
      "authors": "Mehdi Acheli, Walid Gaaloul",
      "abstract": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21064v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21064v1.pdf",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Tool Building as a Path to \"Superintelligence\"",
      "authors": "David Koplow, Tomer Galanti, Tomaso Poggio",
      "abstract": "The Diligent Learner framework suggests LLMs can achieve superintelligence via test-time search, provided a sufficient step-success probability $γ$. In this work, we design a benchmark to measure $γ$ on logical out-of-distribution inference. We construct a class of tasks involving GF(2) circuit reconstruction that grow more difficult with each reasoning step, and that are, from an information-theoretic standpoint, impossible to reliably solve unless the LLM carefully integrates all of the information provided. Our analysis demonstrates that while the $γ$ value for small LLMs declines superlinearly as depth increases, frontier models exhibit partial robustness on this task. Furthermore, we find that successful reasoning at scale is contingent upon precise tool calls, identifying tool design as a critical capability for LLMs to achieve general superintelligence through the Diligent Learner framework.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21061v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21061v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
      "authors": "Anna Martin-Boyle, William Humphreys, Martha Brown, Cara Leckey, Harmanpreet Kaur",
      "abstract": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the assessment strategies of practicing scientists. In collaboration with domain experts, we identified 20 error patterns across seven categories through thematic analysis of 68 question-answer pairs. We validated this schema through contextual inquiries with 10 additional scientists, which showed not only which errors experts naturally identify but also how structured evaluation schemas can help them detect previously overlooked issues. Domain experts use systematic assessment strategies, including technical precision testing, value-based evaluation, and meta-evaluation of their own practices. We discuss implications for supporting expert evaluation of LLM outputs, including opportunities for personalized, schema-driven tools that adapt to individual evaluation patterns and expertise levels.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21059v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21059v1.pdf",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation",
      "authors": "Seongheon Park, Changdae Oh, Hyeong Kyu Choi, Xuefeng Du, Sharon Li",
      "abstract": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21054v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21054v1.pdf",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
      "authors": "Shimin Wen, Zeyu Zhang, Xingdou Bian, Hongjie Zhu, Lulu He, et al.",
      "abstract": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21053v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21053v1.pdf",
      "categories": [
        "cs.CV"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations",
      "authors": "Timur Nabiev, Evgeny Frolov",
      "abstract": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21052v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21052v1.pdf",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Empirically Calibrated Conditional Independence Tests",
      "authors": "Milleno Pan, Antoine de Mathelin, Wesley Tansey",
      "abstract": "Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21036v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21036v1.pdf",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Exchangeable Gaussian Processes for Staggered-Adoption Policy Evaluation",
      "authors": "Hayk Gevorgyan, Konstantinos Kalogeropoulos, Angelos Alexopoulos",
      "abstract": "We study the use of exchangeable multi-task Gaussian processes (GPs) for causal inference in panel data, applying the framework to two settings: one with a single treated unit subject to a once-and-for-all treatment and another with multiple treated units and staggered treatment adoption. Our approach models the joint evolution of outcomes for treated and control units through a GP prior that ensures exchangeability across units while allowing for flexible nonlinear trends over time. The resulting posterior predictive distribution for the untreated potential outcomes of the treated unit provides a counterfactual path, from which we derive pointwise and cumulative treatment effects, along with credible intervals to quantify uncertainty. We implement several variations of the exchangeable GP model using different kernel functions. To assess prediction accuracy, we conduct a placebo-style validation within the pre-intervention window by selecting a ``fake'' intervention date. Ultimately, this study illustrates how exchangeable GPs serve as a flexible tool for policy evaluation in panel data settings and proposes a novel approach to staggered-adoption designs with a large number of treated and control units.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21031v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21031v1.pdf",
      "categories": [
        "stat.ME",
        "stat.ML"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
      "authors": "Christian Simon, MAsato Ishii, Wei-Yao Wang, Koichi Saito, Akio Hayakawa, et al.",
      "abstract": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20981v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20981v1.pdf",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs",
      "authors": "Dhita Putri Pratama, Soyeon Caren Han, Yihao Ding",
      "abstract": "Large Vision-Language Models (LVLMs) achieve strong performance on visual question answering benchmarks, yet often rely on spurious correlations rather than genuine causal reasoning. Existing evaluations primarily assess the correctness of the answers, making it unclear whether failures arise from limited reasoning capability or from misidentifying causally relevant information. We introduce Vision-Language Causal Graphs (VLCGs), a structured, query-conditioned representation that explicitly encodes causally relevant objects, attributes, relations, and scene-grounded assumptions. Building on this representation, we present ViLCaR, a diagnostic benchmark comprising tasks for Causal Attribution, Causal Inference, and Question Answering, along with graph-aligned evaluation metrics that assess relevance identification beyond final answer accuracy. Experiments in state-of-the-art LVLMs show that injecting structured relevance information significantly improves attribution and inference consistency compared to zero-shot and standard in-context learning. These findings suggest that current limitations in LVLM causal reasoning stem primarily from insufficient structural guidance rather than a lack of reasoning capacity.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20878v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20878v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Recursive Belief Vision Language Model",
      "authors": "Vaidehi Bagaria, Bijo Sebastian, Nirav Patel",
      "abstract": "Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20659v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20659v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "When can we trust untrusted monitoring? A safety case sketch across collusion strategies",
      "authors": "Nelson Gardner-Challis, Jonathan Bostock, Georgiy Kozhevnikov, Morgan Sinclaire, Joan Velja, et al.",
      "abstract": "AIs are increasingly being deployed with greater autonomy and capabilities, which increases the risk that a misaligned AI may be able to cause catastrophic harm. Untrusted monitoring -- using one untrusted model to oversee another -- is one approach to reducing risk. Justifying the safety of an untrusted monitoring deployment is challenging because developers cannot safely deploy a misaligned model to test their protocol directly. In this paper, we develop upon existing methods for rigorously demonstrating safety based on pre-deployment testing. We relax assumptions that previous AI control research made about the collusion strategies a misaligned AI might use to subvert untrusted monitoring. We develop a taxonomy covering passive self-recognition, causal collusion (hiding pre-shared signals), acausal collusion (hiding signals via Schelling points), and combined strategies. We create a safety case sketch to clearly present our argument, explicitly state our assumptions, and highlight unsolved challenges. We identify conditions under which passive self-recognition could be a more effective collusion strategy than those studied previously. Our work builds towards more robust evaluations of untrusted monitoring.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20628v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20628v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "CausalReasoningBenchmark: A Real-World Benchmark for Disentangled Evaluation of Causal Identification and Estimation",
      "authors": "Ayush Sawarni, Jiyuan Tan, Vasilis Syrgkanis",
      "abstract": "Many benchmarks for automated causal inference evaluate a system's performance based on a single numerical output, such as an Average Treatment Effect (ATE). This approach conflates two distinct steps in causal analysis: identification-formulating a valid research design under stated assumptions-and estimation-implementing that design numerically on finite data. We introduce CausalReasoningBenchmark, a benchmark of 173 queries across 138 real-world datasets, curated from 85 peer-reviewed research papers and four widely-used causal-inference textbooks. For each query a system must produce (i) a structured identification specification that names the strategy, the treatment, outcome, and control variables, and all design-specific elements, and (ii) a point estimate with a standard error. By scoring these two components separately, our benchmark enables granular diagnosis: it distinguishes failures in causal reasoning from errors in numerical execution. Baseline results with a state-of-the-art LLM show that, while the model correctly identifies the high-level strategy in 84 % of cases, full identification-specification correctness drops to only 30 %, revealing that the bottleneck lies in the nuanced details of research design rather than in computation. CausalReasoningBenchmark is publicly available on Hugging Face and is designed to foster the development of more robust automated causal-inference systems.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20571v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20571v1.pdf",
      "categories": [
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "cc-Shapley: Measuring Multivariate Feature Importance Needs Causal Context",
      "authors": "Jörg Martin, Stefan Haufe",
      "abstract": "Explainable artificial intelligence promises to yield insights into relevant features, thereby enabling humans to examine and scrutinize machine learning models or even facilitating scientific discovery. Considering the widespread technique of Shapley values, we find that purely data-driven operationalization of multivariate feature importance is unsuitable for such purposes. Even for simple problems with two features, spurious associations due to collider bias and suppression arise from considering one feature only in the observational context of the other, which can lead to misinterpretations. Causal knowledge about the data-generating process is required to identify and correct such misleading feature attributions. We propose cc-Shapley (causal context Shapley), an interventional modification of conventional observational Shapley values leveraging knowledge of the data's causal structure, thereby analyzing the relevance of a feature in the causal context of the remaining features. We show theoretically that this eradicates spurious association induced by collider bias. We compare the behavior of Shapley and cc-Shapley values on various, synthetic, and real-world datasets. We observe nullification or reversal of associations compared to univariate feature importance when moving from observational to cc-Shapley.",
      "published": "2026-02-23",
      "link": "http://arxiv.org/abs/2602.20396v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20396v1.pdf",
      "categories": [
        "cs.LG",
        "stat.ME"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Detecting and Mitigating Group Bias in Heterogeneous Treatment Effects",
      "authors": "Joel Persson, Jurriën Bakker, Dennis Bohle, Stefan Feuerriegel, Florian von Wangenheim",
      "abstract": "Heterogeneous treatment effects (HTEs) are increasingly estimated using machine learning models that produce highly personalized predictions of treatment effects. In practice, however, predicted treatment effects are rarely interpreted, reported, or audited at the individual level but, instead, are often aggregated to broader subgroups, such as demographic segments, risk strata, or markets. We show that such aggregation can induce systematic bias of the group-level causal effect: even when models for predicting the individual-level conditional average treatment effect (CATE) are correctly specified and trained on data from randomized experiments, aggregating the predicted CATEs up to the group level does not, in general, recover the corresponding group average treatment effect (GATE). We develop a unified statistical framework to detect and mitigate this form of group bias in randomized experiments. We first define group bias as the discrepancy between the model-implied and experimentally identified GATEs, derive an asymptotically normal estimator, and then provide a simple-to-implement statistical test. For mitigation, we propose a shrinkage-based bias-correction, and show that the theoretically optimal and empirically feasible solutions have closed-form expressions. The framework is fully general, imposes minimal assumptions, and only requires computing sample moments. We analyze the economic implications of mitigating detected group bias for profit-maximizing personalized targeting, thereby characterizing when bias correction alters targeting decisions and profits, and the trade-offs involved. Applications to large-scale experimental data at major digital platforms validate our theoretical results and demonstrate empirical performance.",
      "published": "2026-02-23",
      "link": "http://arxiv.org/abs/2602.20383v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20383v1.pdf",
      "categories": [
        "stat.ME",
        "cs.LG",
        "econ.EM"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
      "authors": "Anna Martin-Boyle, Cara A. C. Leckey, Martha C. Brown, Harmanpreet Kaur",
      "abstract": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source documents into discrete claims and evidence, mapping them to reveal supported assertions, unsupported claims, and information omitted from the source texts. We evaluated PaperTrail in a within-subjects study with 26 researchers who performed two scholarly editing tasks using PaperTrail and a baseline interface. Our results show that PaperTrail significantly lowered participants' trust compared to the baseline. However, this increased caution did not translate to behavioral changes, as people continued to rely on LLM-generated scholarly edits to avoid a cognitively burdensome task. We discuss the value of claim-evidence matching for understanding LLM trustworthiness in scholarly settings, and present design implications for cognition-friendly communication of provenance information.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21045v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21045v1.pdf",
      "categories": [
        "cs.HC",
        "cs.CL"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "Cell-Free Massive MIMO-Assisted SWIPT Using Stacked Intelligent Metasurfaces",
      "authors": "Thien Duc Hua, Mohammadali Mohammadi, Hien Quoc Ngo, Michail Matthaiou",
      "abstract": "This study explores a next-generation multiple access (NGMA) framework for cell-free massive MIMO (CF-mMIMO) systems enhanced by stacked intelligent metasurfaces (SIMs), aiming to improve simultaneous wireless information and power transfer (SWIPT) performance. A fundamental challenge lies in optimally selecting the operating modes of access points (APs) to jointly maximize the received energy and satisfy spectral efficiency (SE) quality-of-service constraints. Practical system impairments, including a non-linear harvested energy model, pilot contamination (PC), channel estimation errors, and reliance on long-term statistical channel state information (CSI), are considered. We derive closed-form expressions for both the achievable SE and the average sum harvested energy (sum-HE). A mixed-integer non-convex optimization problem is formulated to jointly optimize the SIM phase shifts, APs mode selection, and power allocation to maximize average sum-HE under SE and average harvested energy constraints. To solve this problem, we propose a centralized training, decentralized execution (CTDE) framework based on deep reinforcement learning (DRL), which efficiently handles high-dimensional decision spaces. A Markovian environment and a normalized joint reward function are introduced to enhance the training stability across on-policy and off-policy DRL algorithms. Additionally, we provide a two-phase convex-based solution as a theoretical robust performance. Numerical results demonstrate that the proposed DRL-based CTDE framework achieves SWIPT performance comparable to convexification-based solution, while significantly outperforming baselines.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20983v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20983v1.pdf",
      "categories": [
        "eess.SP",
        "cs.IT"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
      "authors": "Taiqiang Wu, Zenan Zu, Bo Zhou, Ngai Wong",
      "abstract": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20945v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20945v1.pdf",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning",
      "authors": "Dimitrios Dimou, José Santos-Victor, Plinio Moreno",
      "abstract": "In this paper, we address the problem of task-oriented grasping for humanoid robots, emphasizing the need to align with human social norms and task-specific objectives. Existing methods, employ a variety of open-loop and closed-loop approaches but lack an end-to-end solution that can grasp several objects while taking into account the downstream task's constraints. Our proposed approach employs reinforcement learning to enhance task-oriented grasping, prioritizing the post-grasp intention of the agent. We extract human grasp preferences from the ContactPose dataset, and train a hand synergy model based on the Variational Autoencoder (VAE) to imitate the participant's grasping actions. Based on this data, we train an agent able to grasp multiple objects while taking into account distinct post-grasp intentions that are task-specific. By combining data-driven insights from human grasping behavior with learning by exploration provided by reinforcement learning, we can develop humanoid robots capable of context-aware manipulation actions, facilitating collaboration in human-centered environments.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20915v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20915v1.pdf",
      "categories": [
        "cs.RO"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
      "authors": "Jihao Qiu, Lingxi Xie, Xinyue Huo, Qi Tian, Qixiang Ye",
      "abstract": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.20913v1",
      "pdfLink": "http://arxiv.org/pdf/2602.20913v1.pdf",
      "categories": [
        "cs.CV"
      ],
      "topic": "Reinforcement Learning"
    }
  ]
}