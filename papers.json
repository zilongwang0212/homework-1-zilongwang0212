{
  "lastUpdated": "2026-02-26T02:41:27.144998",
  "topics": {
    "ai_ml": {
      "name": "AI & Machine Learning",
      "count": 10
    },
    "causal": {
      "name": "Causal Inference",
      "count": 10
    },
    "llm": {
      "name": "Large Language Models",
      "count": 5
    },
    "reinforcement": {
      "name": "Reinforcement Learning",
      "count": 5
    }
  },
  "papers": [
    {
      "title": "TG-ASR: Translation-Guided Learning with Parallel Gated Cross Attention for Low-Resource Automatic Speech Recognition",
      "authors": "Cheng-Yeh Yang, Chien-Chun Wang, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang, et al.",
      "abstract": "Low-resource automatic speech recognition (ASR) continues to pose significant challenges, primarily due to the limited availability of transcribed data for numerous languages. While a wealth of spoken content is accessible in television dramas and online videos, Taiwanese Hokkien exemplifies this issue, with transcriptions often being scarce and the majority of available subtitles provided only in Mandarin. To address this deficiency, we introduce TG-ASR for Taiwanese Hokkien drama speech recognition, a translation-guided ASR framework that utilizes multilingual translation embeddings to enhance recognition performance in low-resource environments. The framework is centered around the parallel gated cross-attention (PGCA) mechanism, which adaptively integrates embeddings from various auxiliary languages into the ASR decoder. This mechanism facilitates robust cross-linguistic semantic guidance while ensuring stable optimization and minimizing interference between languages. To support ongoing research initiatives, we present YT-THDC, a 30-hour corpus of Taiwanese Hokkien drama speech with aligned Mandarin subtitles and manually verified Taiwanese Hokkien transcriptions. Comprehensive experiments and analyses identify the auxiliary languages that most effectively enhance ASR performance, achieving a 14.77% relative reduction in character error rate and demonstrating the efficacy of translation-guided learning for underrepresented languages in practical applications.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22039v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22039v1.pdf",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "RT-RMOT: A Dataset and Framework for RGB-Thermal Referring Multi-Object Tracking",
      "authors": "Yanqiu Yu, Zhifan Jin, Sijia Chen, Tongfei Chu, En Yu, et al.",
      "abstract": "Referring Multi-Object Tracking has attracted increasing attention due to its human-friendly interactive characteristics, yet it exhibits limitations in low-visibility conditions, such as nighttime, smoke, and other challenging scenarios. To overcome this limitation, we propose a new RGB-Thermal RMOT task, named RT-RMOT, which aims to fuse RGB appearance features with the illumination robustness of the thermal modality to enable all-day referring multi-object tracking. To promote research on RT-RMOT, we construct the first Referring Multi-Object Tracking dataset under RGB-Thermal modality, named RefRT. It contains 388 language descriptions, 1,250 tracked targets, and 166,147 Language-RGB-Thermal (L-RGB-T) triplets. Furthermore, we propose RTrack, a framework built upon a multimodal large language model (MLLM) that integrates RGB, thermal, and textual features. Since the initial framework still leaves room for improvement, we introduce a Group Sequence Policy Optimization (GSPO) strategy to further exploit the model's potential. To alleviate training instability during RL fine-tuning, we introduce a Clipped Advantage Scaling (CAS) strategy to suppress gradient explosion. In addition, we design Structured Output Reward and Comprehensive Detection Reward to balance exploration and exploitation, thereby improving the completeness and accuracy of target perception. Extensive experiments on the RefRT dataset demonstrate the effectiveness of the proposed RTrack framework.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22033v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22033v1.pdf",
      "categories": [
        "cs.CV"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "RGB-Event HyperGraph Prompt for Kilometer Marker Recognition based on Pre-trained Foundation Models",
      "authors": "Xiaoyu Xian, Shiao Wang, Xiao Wang, Daxin Tian, Yan Tian",
      "abstract": "Metro trains often operate in highly complex environments, characterized by illumination variations, high-speed motion, and adverse weather conditions. These factors pose significant challenges for visual perception systems, especially those relying solely on conventional RGB cameras. To tackle these difficulties, we explore the integration of event cameras into the perception system, leveraging their advantages in low-light conditions, high-speed scenarios, and low power consumption. Specifically, we focus on Kilometer Marker Recognition (KMR), a critical task for autonomous metro localization under GNSS-denied conditions. In this context, we propose a robust baseline method based on a pre-trained RGB OCR foundation model, enhanced through multi-modal adaptation. Furthermore, we construct the first large-scale RGB-Event dataset, EvMetro5K, containing 5,599 pairs of synchronized RGB-Event samples, split into 4,479 training and 1,120 testing samples. Extensive experiments on EvMetro5K and other widely used benchmarks demonstrate the effectiveness of our approach for KMR. Both the dataset and source code will be released on https://github.com/Event-AHU/EvMetro5K_benchmark",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22026v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22026v1.pdf",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Disease Progression and Subtype Modeling for Combined Discrete and Continuous Input Data",
      "authors": "Sterre de Jonge, Elisabeth J. Vinke, Meike W. Vernooij, Daniel C. Alexander, Alexandra L. Young, et al.",
      "abstract": "Disease progression modeling provides a robust framework to identify long-term disease trajectories from short-term biomarker data. It is a valuable tool to gain a deeper understanding of diseases with a long disease trajectory, such as Alzheimer's disease. A key limitation of most disease progression models is that they are specific to a single data type (e.g., continuous data), thereby limiting their applicability to heterogeneous, real-world datasets. To address this limitation, we propose the Mixed Events model, a novel disease progression model that handles both discrete and continuous data types. This model is implemented within the Subtype and Stage Inference (SuStaIn) framework, resulting in Mixed-SuStaIn, enabling subtype and progression modeling. We demonstrate the effectiveness of Mixed-SuStaIn through simulation experiments and real-world data from the Alzheimer's Disease Neuroimaging Initiative, showing that it performs well on mixed datasets. The code is available at: https://github.com/ucl-pond/pySuStaIn.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22018v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22018v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Function-Space Empirical Bayes Regularisation with Student's t Priors",
      "authors": "Pengcheng Hao, Ercan Engin Kuruoglu",
      "abstract": "Bayesian deep learning (BDL) has emerged as a principled approach to produce reliable uncertainty estimates by integrating deep neural networks with Bayesian inference, and the selection of informative prior distributions remains a significant challenge. Various function-space variational inference (FSVI) regularisation methods have been presented, assigning meaningful priors over model predictions. However, these methods typically rely on a Gaussian prior, which fails to capture the heavy-tailed statistical characteristics inherent in neural network outputs. By contrast, this work proposes a novel function-space empirical Bayes regularisation framework -- termed ST-FS-EB -- which employs heavy-tailed Student's $t$ priors in both parameter and function spaces. Also, we approximate the posterior distribution through variational inference (VI), inducing an evidence lower bound (ELBO) objective based on Monte Carlo (MC) dropout. Furthermore, the proposed method is evaluated against various VI-based BDL baselines, and the results demonstrate its robust performance in in-distribution prediction, out-of-distribution (OOD) detection and handling distribution shifts.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22015v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22015v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "A Diversity Diet for a Healthier Model: A Case Study of French ModernBERT",
      "authors": "Louis Estève, Christophe Servan, Thomas Lavergne, Agata Savary",
      "abstract": "Diversity has been gaining interest in the NLP community in recent years. At the same time, state-of-the-art transformer models such as ModernBERT use very large pre-training datasets, which are driven by size rather than by diversity. This summons for an investigation of the impact of diversity on the ModernBERT pre-training. We do so in this study, with the express intent of reducing pre-training dataset size, while retaining at least comparable performance. We compare diversity-driven sampling algorithms, so as to pick the best one. We find that diversity-driven sampling allows in some tasks to gain 10 points relative to randomly-sampled pre-training data of commensurate size. We also see that a model pre-trained for 483h on a diversity-driven dataset of 150M tokens can yield a commensurate performance to a model pre-trained for 1,775h on a randomly-driven dataset of 2.4B tokens.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22014v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22014v1.pdf",
      "categories": [
        "cs.CL"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "RobustVisRAG: Causality-Aware Vision-Based Retrieval-Augmented Generation under Visual Degradations",
      "authors": "I-Hsiang Chen, Yu-Wei Liu, Tse-Yu Wu, Yu-Chien Chiang, Jen-Chien Yang, et al.",
      "abstract": "Vision-based Retrieval-Augmented Generation (VisRAG) leverages vision-language models (VLMs) to jointly retrieve relevant visual documents and generate grounded answers based on multimodal evidence. However, existing VisRAG models degrade in performance when visual inputs suffer from distortions such as blur, noise, low light, or shadow, where semantic and degradation factors become entangled within pretrained visual encoders, leading to errors in both retrieval and generation stages. To address this limitation, we introduce RobustVisRAG, a causality-guided dual-path framework that improves VisRAG robustness while preserving efficiency and zero-shot generalization. RobustVisRAG uses a non-causal path to capture degradation signals through unidirectional attention and a causal path to learn purified semantics guided by these signals. Together with the proposed Non-Causal Distortion Modeling and Causal Semantic Alignment objectives, the framework enforces a clear separation between semantics and degradations, enabling stable retrieval and generation under challenging visual conditions. To evaluate robustness under realistic conditions, we introduce the Distortion-VisRAG dataset, a large-scale benchmark containing both synthetic and real-world degraded documents across seven domains, with 12 synthetic and 5 real distortion types that comprehensively reflect practical visual degradations. Experimental results show that RobustVisRAG improves retrieval, generation, and end-to-end performance by 7.35%, 6.35%, and 12.40%, respectively, on real-world degradations, while maintaining comparable accuracy on clean inputs.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22013v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22013v1.pdf",
      "categories": [
        "cs.CV"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "World Guidance: World Modeling in Condition Space for Action Generation",
      "authors": "Yue Su, Sijin Chen, Haixin Shi, Mingyu Liu, Zhengshen Zhang, et al.",
      "abstract": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22010v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22010v1.pdf",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Neural solver for Wasserstein Geodesics and optimal transport dynamics",
      "authors": "Hailiang Liu, Yan-Han Chen",
      "abstract": "In recent years, the machine learning community has increasingly embraced the optimal transport (OT) framework for modeling distributional relationships. In this work, we introduce a sample-based neural solver for computing the Wasserstein geodesic between a source and target distribution, along with the associated velocity field. Building on the dynamical formulation of the optimal transport (OT) problem, we recast the constrained optimization as a minimax problem, using deep neural networks to approximate the relevant functions. This approach not only provides the Wasserstein geodesic but also recovers the OT map, enabling direct sampling from the target distribution. By estimating the OT map, we obtain velocity estimates along particle trajectories, which in turn allow us to learn the full velocity field. The framework is flexible and readily extends to general cost functions, including the commonly used quadratic cost. We demonstrate the effectiveness of our method through experiments on both synthetic and real datasets.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22003v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22003v1.pdf",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code",
      "authors": "WeiZhe Xu, Mengyu Liu, Fanxin Kong",
      "abstract": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21997v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21997v1.pdf",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "topic": "AI & Machine Learning"
    },
    {
      "title": "2-Step Agent: A Framework for the Interaction of a Decision Maker with AI Decision Support",
      "authors": "Otto Nyberg, Fausto Carcassi, Giovanni Cinà",
      "abstract": "Across a growing number of fields, human decision making is supported by predictions from AI models. However, we still lack a deep understanding of the effects of adoption of these technologies. In this paper, we introduce a general computational framework, the 2-Step Agent, which models the effects of AI-assisted decision making. Our framework uses Bayesian methods for causal inference to model 1) how a prediction on a new observation affects the beliefs of a rational Bayesian agent, and 2) how this change in beliefs affects the downstream decision and subsequent outcome. Using this framework, we show by simulations how a single misaligned prior belief can be sufficient for decision support to result in worse downstream outcomes compared to no decision support. Our results reveal several potential pitfalls of AI-driven decision support and highlight the need for thorough model documentation and proper user training.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21889v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21889v1.pdf",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Easy to Learn, Yet Hard to Forget: Towards Robust Unlearning Under Bias",
      "authors": "JuneHyoung Kwon, MiHyeon Kim, Eunju Lee, Yoonji Lee, Seunghoon Lee, et al.",
      "abstract": "Machine unlearning, which enables a model to forget specific data, is crucial for ensuring data privacy and model reliability. However, its effectiveness can be severely undermined in real-world scenarios where models learn unintended biases from spurious correlations within the data. This paper investigates the unique challenges of unlearning from such biased models. We identify a novel phenomenon we term ``shortcut unlearning,\" where models exhibit an ``easy to learn, yet hard to forget\" tendency. Specifically, models struggle to forget easily-learned, bias-aligned samples; instead of forgetting the class attribute, they unlearn the bias attribute, which can paradoxically improve accuracy on the class intended to be forgotten. To address this, we propose CUPID, a new unlearning framework inspired by the observation that samples with different biases exhibit distinct loss landscape sharpness. Our method first partitions the forget set into causal- and bias-approximated subsets based on sample sharpness, then disentangles model parameters into causal and bias pathways, and finally performs a targeted update by routing refined causal and bias gradients to their respective pathways. Extensive experiments on biased datasets including Waterbirds, BAR, and Biased NICO++ demonstrate that our method achieves state-of-the-art forgetting performance and effectively mitigates the shortcut unlearning problem.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21773v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21773v1.pdf",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "TiMi: Empower Time Series Transformers with Multimodal Mixture of Experts",
      "authors": "Jiafeng Lin, Yuxuan Wang, Huakun Luo, Zhongyi Pei, Jianmin Wang",
      "abstract": "Multimodal time series forecasting has garnered significant attention for its potential to provide more accurate predictions than traditional single-modality models by leveraging rich information inherent in other modalities. However, due to fundamental challenges in modality alignment, existing methods often struggle to effectively incorporate multimodal data into predictions, particularly textual information that has a causal influence on time series fluctuations, such as emergency reports and policy announcements. In this paper, we reflect on the role of textual information in numerical forecasting and propose Time series transformers with Multimodal Mixture-of-Experts, TiMi, to unleash the causal reasoning capabilities of LLMs. Concretely, TiMi utilizes LLMs to generate inferences on future developments, which serve as guidance for time series forecasting. To seamlessly integrate both exogenous factors and time series into predictions, we introduce a Multimodal Mixture-of-Experts (MMoE) module as a lightweight plug-in to empower Transformer-based time series models for multimodal forecasting, eliminating the need for explicit representation-level alignment. Experimentally, our proposed TiMi demonstrates consistent state-of-the-art performance on sixteen real-world multimodal forecasting benchmarks, outperforming advanced baselines while offering both strong adaptability and interpretability.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21693v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21693v1.pdf",
      "categories": [
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "One Brain, Omni Modalities: Towards Unified Non-Invasive Brain Decoding with Large Language Models",
      "authors": "Changli Tang, Shurui Li, Junliang Wang, Qinfan Xiao, Zhonghao Zhai, et al.",
      "abstract": "Deciphering brain function through non-invasive recordings requires synthesizing complementary high-frequency electromagnetic (EEG/MEG) and low-frequency metabolic (fMRI) signals. However, despite their shared neural origins, extreme discrepancies have traditionally confined these modalities to isolated analysis pipelines, hindering a holistic interpretation of brain activity. To bridge this fragmentation, we introduce \\textbf{NOBEL}, a \\textbf{n}euro-\\textbf{o}mni-modal \\textbf{b}rain-\\textbf{e}ncoding \\textbf{l}arge language model (LLM) that unifies these heterogeneous signals within the LLM's semantic embedding space. Our architecture integrates a unified encoder for EEG and MEG with a novel dual-path strategy for fMRI, aligning non-invasive brain signals and external sensory stimuli into a shared token space, then leverages an LLM as a universal backbone. Extensive evaluations demonstrate that NOBEL serves as a robust generalist across standard single-modal tasks. We also show that the synergistic fusion of electromagnetic and metabolic signals yields higher decoding accuracy than unimodal baselines, validating the complementary nature of multiple neural modalities. Furthermore, NOBEL exhibits strong capabilities in stimulus-aware decoding, effectively interpreting visual semantics from multi-subject fMRI data on the NSD and HAD datasets while uniquely leveraging direct stimulus inputs to verify causal links between sensory signals and neural responses. NOBEL thus takes a step towards unifying non-invasive brain decoding, demonstrating the promising potential of omni-modal brain understanding.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21522v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21522v1.pdf",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.CL"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "A Researcher's Guide to Empirical Risk Minimization",
      "authors": "Lars van der Laan",
      "abstract": "This guide develops high-probability regret bounds for empirical risk minimization (ERM). The presentation is modular: we state broadly applicable guarantees under high-level conditions and give tools for verifying them for specific losses and function classes. We emphasize that many ERM rate derivations can be organized around a three-step recipe -- a basic inequality, a uniform local concentration bound, and a fixed-point argument -- which yields regret bounds in terms of a critical radius, defined via localized Rademacher complexity, under a mild Bernstein-type variance--risk condition. To make these bounds concrete, we upper bound the critical radius using local maximal inequalities and metric-entropy integrals, recovering familiar rates for VC-subgraph, Sobolev/Hölder, and bounded-variation classes.   We also review ERM with nuisance components -- including weighted ERM and Neyman-orthogonal losses -- as they arise in causal inference, missing data, and domain adaptation. Following the orthogonal learning framework, we highlight that these problems often admit regret-transfer bounds linking regret under an estimated loss to population regret under the target loss. These bounds typically decompose regret into (i) statistical error under the estimated (optimized) loss and (ii) approximation error due to nuisance estimation. Under sample splitting or cross-fitting, the first term can be controlled using standard fixed-loss ERM regret bounds, while the second term depends only on nuisance-estimation accuracy. We also treat the in-sample regime, where nuisances and the ERM are fit on the same data, deriving regret bounds and giving sufficient conditions for fast rates.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21501v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21501v1.pdf",
      "categories": [
        "stat.ML",
        "cs.LG",
        "math.ST"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Causal Decoding for Hallucination-Resistant Multimodal Large Language Models",
      "authors": "Shiwei Tan, Hengyi Wang, Weiyi Qin, Qi Xu, Zhigang Hua, et al.",
      "abstract": "Multimodal Large Language Models (MLLMs) deliver detailed responses on vision-language tasks, yet remain susceptible to object hallucination (introducing objects not present in the image), undermining reliability in practice. Prior efforts often rely on heuristic penalties, post-hoc correction, or generic decoding tweaks, which do not directly intervene in the mechanisms that trigger object hallucination and thus yield limited gains. To address this challenge, we propose a causal decoding framework that applies targeted causal interventions during generation to curb spurious object mentions. By reshaping the decoding dynamics to attenuate spurious dependencies, our approach reduces false object tokens while maintaining descriptive quality. Across captioning and QA benchmarks, our framework substantially lowers object-hallucination rates and achieves state-of-the-art faithfulness without degrading overall output quality.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21441v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21441v1.pdf",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "PSF-Med: Measuring and Explaining Paraphrase Sensitivity in Medical Vision Language Models",
      "authors": "Binesh Sadanandan, Vahid Behzadan",
      "abstract": "Medical Vision Language Models (VLMs) can change their answers when clinicians rephrase the same question, which raises deployment risks. We introduce Paraphrase Sensitivity Failure (PSF)-Med, a benchmark of 19,748 chest Xray questions paired with about 92,000 meaningpreserving paraphrases across MIMIC-CXR and PadChest. Across six medical VLMs, we measure yes/no flips for the same image and find flip rates from 8% to 58%. However, low flip rate does not imply visual grounding: text-only baselines show that some models stay consistent even when the image is removed, suggesting they rely on language priors. To study mechanisms in one model, we apply GemmaScope 2 Sparse Autoencoders (SAEs) to MedGemma 4B and analyze FlipBank, a curated set of 158 flip cases. We identify a sparse feature at layer 17 that correlates with prompt framing and predicts decision margin shifts. In causal patching, removing this feature's contribution recovers 45% of the yesminus-no logit margin on average and fully reverses 15% of flips. Acting on this finding, we show that clamping the identified feature at inference reduces flip rates by 31% relative with only a 1.3 percentage-point accuracy cost, while also decreasing text-prior reliance. These results suggest that flip rate alone is not enough; robustness evaluations should test both paraphrase stability and image reliance.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21428v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21428v1.pdf",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Alignment-Weighted DPO: A principled reasoning approach to improve safety alignment",
      "authors": "Mengxuan Hu, Vivek V. Datla, Anoop Kumar, Zihan Guan, Sheng Li, et al.",
      "abstract": "Recent advances in alignment techniques such as Supervised Fine-Tuning (SFT), Reinforcement Learning from Human Feedback (RLHF), and Direct Preference Optimization (DPO) have improved the safety of large language models (LLMs). However, these LLMs remain vulnerable to jailbreak attacks that disguise harmful intent through indirect or deceptive phrasing. Using causal intervention, we empirically demonstrate that this vulnerability stems from shallow alignment mechanisms that lack deep reasoning, often rejecting harmful prompts without truly understanding why they are harmful. To mitigate this vulnerability, we propose enhancing alignment through reasoning-aware post-training. We construct and release a novel Chain-of-Thought (CoT) fine-tuning dataset that includes both utility-oriented and safety-critical prompts with step-by-step rationales. Fine-tuning on this dataset encourages models to produce principled refusals grounded in reasoning, outperforming standard SFT baselines. Furthermore, inspired by failure patterns in CoT fine-tuning, we introduce Alignment-Weighted DPO, which targets the most problematic parts of an output by assigning different preference weights to the reasoning and final-answer segments. This produces finer-grained, targeted updates than vanilla DPO and improves robustness to diverse jailbreak strategies. Extensive experiments across multiple safety and utility benchmarks show that our method consistently improves alignment robustness while maintaining overall model utility.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21346v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21346v1.pdf",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations",
      "authors": "Timur Nabiev, Evgeny Frolov",
      "abstract": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21052v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21052v1.pdf",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Empirically Calibrated Conditional Independence Tests",
      "authors": "Milleno Pan, Antoine de Mathelin, Wesley Tansey",
      "abstract": "Conditional independence tests (CIT) are widely used for causal discovery and feature selection. Even with false discovery rate (FDR) control procedures, they often fail to provide frequentist guarantees in practice. We highlight two common failure modes: (i) in small samples, asymptotic guarantees for many CITs can be inaccurate and even correctly specified models fail to estimate the noise levels and control the error, and (ii) when sample sizes are large but models are misspecified, unaccounted dependencies skew the test's behavior and fail to return uniform p-values under the null. We propose Empirically Calibrated Conditional Independence Tests (ECCIT), a method that measures and corrects for miscalibration. For a chosen base CIT (e.g., GCM, HRT), ECCIT optimizes an adversary that selects features and response functions to maximize a miscalibration metric. ECCIT then fits a monotone calibration map that adjusts the base-test p-values in proportion to the observed miscalibration. Across empirical benchmarks on synthetic and real data, ECCIT achieves valid FDR with higher power than existing calibration strategies while remaining test agnostic.",
      "published": "2026-02-24",
      "link": "http://arxiv.org/abs/2602.21036v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21036v1.pdf",
      "categories": [
        "stat.ME",
        "cs.LG",
        "stat.ML"
      ],
      "topic": "Causal Inference"
    },
    {
      "title": "Detecting UX smells in Visual Studio Code using LLMs",
      "authors": "Andrés Rodriguez, Juan Cruz Gardey, Alejandra Garrido",
      "abstract": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22020v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22020v1.pdf",
      "categories": [
        "cs.SE",
        "cs.HC"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs",
      "authors": "Chris Egersdoerfer, Arnav Sareen, Jean Luca Bez, Suren Byna,  Dongkuan, et al.",
      "abstract": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22017v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22017v1.pdf",
      "categories": [
        "cs.DC"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "Are Foundation Models the Route to Full-Stack Transfer in Robotics?",
      "authors": "Freek Stulp, Samuel Bustamante, João Silvério, Alin Albu-Schäffer, Jeannette Bohg, et al.",
      "abstract": "In humans and robots alike, transfer learning occurs at different levels of abstraction, from high-level linguistic transfer to low-level transfer of motor skills. In this article, we provide an overview of the impact that foundation models and transformer networks have had on these different levels, bringing robots closer than ever to \"full-stack transfer\". Considering LLMs, VLMs and VLAs from a robotic transfer learning perspective allows us to highlight recurring concepts for transfer, beyond specific implementations. We also consider the challenges of data collection and transfer benchmarks for robotics in the age of foundation models. Are foundation models the route to full-stack transfer in robotics? Our expectation is that they will certainly stay on this route as a key technology.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.22001v1",
      "pdfLink": "http://arxiv.org/pdf/2602.22001v1.pdf",
      "categories": [
        "cs.RO"
      ],
      "topic": "Large Language Models"
    },
    {
      "title": "PanoEnv: Exploring 3D Spatial Intelligence in Panoramic Environments with Reinforcement Learning",
      "authors": "Zekai Lin, Xu Zheng",
      "abstract": "360 panoramic images are increasingly used in virtual reality, autonomous driving, and robotics for holistic scene understanding. However, current Vision-Language Models (VLMs) struggle with 3D spatial reasoning on Equirectangular Projection (ERP) images due to geometric distortion and limited 3D supervision. We introduce PanoEnv, a large-scale VQA benchmark built from synthetic 3D environments, containing 14.8K questions across five categories (e.g., relative position, volume comparison) grounded in accurate 3D annotations including depth, segmentation, and bounding boxes. Benchmarking 14 state-of-the-art VLMs reveals limited 3D understanding, achieving only 49.34% overall accuracy and 8.36% on open-ended (OE) questions. To enhance 3D reasoning, we propose a reinforcement learning post-training framework based on Group Relative Policy Optimization (GRPO) with a ground-truth-guided reward that incorporates five geometry-aware strategies such as distance tolerance and spatial consistency. A two-stage curriculum further mitigates catastrophic forgetting: Stage 1 trains on structured tasks (true/false and multiple choice), and Stage 2 fine-tunes on mixed open-ended data to improve generalization. Our 7B model achieves new state-of-the-art performance, improving overall accuracy to 52.93% (+3.59%) and open-ended accuracy to 14.83% while maintaining structured-task performance. It also achieves top semantic evaluation scores (Q-Score 6.24, P-Score 5.95), surpassing 32B models. These results demonstrate that PanoEnv-QA and our curriculum-based RL framework effectively instill 3D spatial intelligence in VLMs for omnidirectional perception.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21992v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21992v1.pdf",
      "categories": [
        "cs.CV"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "RADAR: Reasoning as Discrimination with Aligned Representations for LLM-based Knowledge Graph Reasoning",
      "authors": "Bo Xue, Yuan Jin, Luoyi Fu, Jiaxin Ding, Xinbing Wang",
      "abstract": "Knowledge graph reasoning (KGR) infers missing facts, with recent advances increasingly harnessing the semantic priors and reasoning abilities of Large Language Models (LLMs). However, prevailing generative paradigms are prone to memorizing surface-level co-occurrences rather than learning genuine relational semantics, limiting out-of-distribution generalization. To address this, we propose RADAR, which reformulates KGR from generative pattern matching to discriminative relational reasoning. We recast KGR as discriminative entity selection, where reinforcement learning enforces relative entity separability beyond token-likelihood imitation. Leveraging this separability, inference operates directly in representation space, ensuring consistency with the discriminative optimization and bypassing generation-induced hallucinations. Across four benchmarks, RADAR achieves 5-6% relative gains on link prediction and triple classification over strong LLM baselines, while increasing task-relevant mutual information in intermediate representations by 62.9%, indicating more robust and transferable relational reasoning.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21951v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21951v1.pdf",
      "categories": [
        "cs.CL"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "ExpLang: Improved Exploration and Exploitation in LLM Reasoning with On-Policy Thinking Language Selection",
      "authors": "Changjiang Gao, Zixian Huang, Kaichen Yang, Jiajun Chen, Jixing Li, et al.",
      "abstract": "Current large reasoning models (LRMs) have shown strong ability on challenging tasks after reinforcement learning (RL) based post-training. However, previous work mainly focuses on English reasoning in expectation of the strongest performance, despite the demonstrated potential advantage of multilingual thinking, as well as the requirement for native thinking traces by global users. In this paper, we propose ExpLang, a novel LLM post-training pipeline that enables on-policy thinking language selection to improve exploration and exploitation during RL with the use of multiple languages. The results show that our method steadily outperforms English-only training with the same training budget, while showing high thinking language compliance for both seen and unseen languages. Analysis shows that, by enabling on-policy thinking language selection as an action during RL, ExpLang effectively extends the RL exploration space with diversified language preference and improves the RL exploitation outcome with leveraged non-English advantage. The method is orthogonal to most RL algorithms and opens up a new perspective on using multilinguality to improve LRMs.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21887v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21887v1.pdf",
      "categories": [
        "cs.CL"
      ],
      "topic": "Reinforcement Learning"
    },
    {
      "title": "Distill and Align Decomposition for Enhanced Claim Verification",
      "authors": "Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, Fernando Acero, et al.",
      "abstract": "Complex claim verification requires decomposing sentences into verifiable subclaims, yet existing methods struggle to align decomposition quality with verification performance. We propose a reinforcement learning (RL) approach that jointly optimizes decomposition quality and verifier alignment using Group Relative Policy Optimization (GRPO). Our method integrates: (i) structured sequential reasoning; (ii) supervised finetuning on teacher-distilled exemplars; and (iii) a multi-objective reward balancing format compliance, verifier alignment, and decomposition quality. Across six evaluation settings, our trained 8B decomposer improves downstream verification performance to (71.75%) macro-F1, outperforming prompt-based approaches ((+1.99), (+6.24)) and existing RL methods ((+5.84)). Human evaluation confirms the high quality of the generated subclaims. Our framework enables smaller language models to achieve state-of-the-art claim verification by jointly optimising for verification accuracy and decomposition quality.",
      "published": "2026-02-25",
      "link": "http://arxiv.org/abs/2602.21857v1",
      "pdfLink": "http://arxiv.org/pdf/2602.21857v1.pdf",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "topic": "Reinforcement Learning"
    }
  ]
}